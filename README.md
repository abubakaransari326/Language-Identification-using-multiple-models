# ğŸŒ Language Identification Using RoBERTa and Comparative Models

This project was developed as part of **CS 584: Natural Language Processing** at **Stevens Institute of Technology**. It explores the task of multilingual language identification using a variety of models, including traditional machine learning classifiers, transformer-based architectures, and large language models.

---

## ğŸ“š Dataset

- **WiLI-2018 (Wikipedia Language Identification)**
- 235 languages, each with:
  - 500 training samples
  - 500 test samples
- Each text is a short excerpt from Wikipedia
- Labels follow the ISO 639-3 format (e.g., `eng`, `deu`, `hin`)
- Source: [WiLI Dataset on Hugging Face](https://huggingface.co/datasets/wili_2018)

---

## ğŸ§  Models Implemented

| Model        | Type                  | Description                                      |
|--------------|-----------------------|--------------------------------------------------|
| Naive Bayes  | Traditional ML         | Fast, TF-IDF based baseline                      |
| SVM          | Traditional ML         | High accuracy, linear kernel                    |
| BERT         | Transformer            | Fine-tuned on WiLI using Hugging Face           |
| RoBERTa      | Transformer            | Best-performing transformer in this task        |
| ChatGPT      | LLM (API-based)        | Zero-shot classification using gpt-3.5-turbo    |

---

## âš™ï¸ Project Structure

Language-Identification-using-multiple-models/
â”‚
â”œâ”€â”€ notebooks/ # Jupyter notebooks for each model
â”‚ â”œâ”€â”€ RoBERTa.ipynb
â”‚ â”œâ”€â”€ BERT.ipynb
â”‚ â”œâ”€â”€ SVM.ipynb
â”‚ â”œâ”€â”€ Naive bayes.ipynb
â”‚ â””â”€â”€ ChatGPT.ipynb
â”‚
â”œâ”€â”€ results/ # Confusion matrices, result plots
â”‚ â”œâ”€â”€ *.txt
â”‚ â””â”€â”€ *.png
â”‚
â”œâ”€â”€ WiLI/ # Dataset files (not tracked in Git)
â”‚ â”œâ”€â”€ x_train.txt
â”‚ â”œâ”€â”€ y_train.txt
â”‚ â”œâ”€â”€ x_test.txt
â”‚ â”œâ”€â”€ y_test.txt
â”‚ â””â”€â”€ labels.csv
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ LICENSE # MIT License
â””â”€â”€ README.md # You are here!



---

## ğŸ“Š Evaluation Metrics

- Accuracy
- Precision, Recall, F1-Score (Macro)
- Confusion Matrix (Top-20 error pairs)
- Runtime (training/inference duration)

---

## ğŸ† Results Summary

| Model        | Accuracy | F1 Score | Runtime             |
|--------------|----------|----------|---------------------|
| SVM          | 96.07%   | 0.960    | ~32 minutes         |
| RoBERTa      | 94.7%    | 0.950    | ~3.6 hours          |
| BERT         | 93.57%   | 0.941    | ~3.5 hours          |
| Naive Bayes  | 92.66%   | 0.930    | 15.1 seconds        |
| ChatGPT      | 58.46%   | 0.394    | ~12 hours           |

---

## ğŸ–¼ï¸ Confusion Matrix (RoBERTa)

![RoBERTa Confusion Matrix](results/roberta_confusion_matrix_top20.png)

---

## ğŸš€ How to Run

1. Clone the repo:
```bash
git clone https://github.com/abubakaransari326/Language-Identification-using-multiple-models.git
cd Language-Identification-using-multiple-models

2. Install Dependencies:
pip install -r requirements.txt

3. Run any notebook inside notebooks/
âš ï¸ Note: Dataset files are not included in this repo. Place x_train.txt, y_train.txt, x_test.txt, and y_test.txt in the WiLI/ folder.


ğŸ‘¤ Author
Abu Bakar Noor Ul Hassan
Stevens Institute of Technology